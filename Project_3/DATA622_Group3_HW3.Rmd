---
title: "CUNY SPS DATA 622 - Machine Learning and Big Data"
subtitle: 'Spring 2021 - Group 3 - Homework 3'
author: "Maryluz Cruz, Samantha Deokinanan, Amber Ferger, Tony Mei, and Charlie Rosemond"
date: "`r format(Sys.time(), '%B %d, %Y')`"
output:
  html_document
urlcolor: purple
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo=FALSE, error = FALSE, warning = FALSE, message = FALSE, fig.align = "center")
```

## R Packages

The statistical tool that will be used to facilitate the modeling of the data was `R`. The main packages used for data wrangling, visualization, and graphics were listed below.

```{r libraries, echo=TRUE}
# Required R packages
library(palmerpenguins)
library(tidyverse)
library(kableExtra)
library(summarytools)
library(GGally)
library(caret)
library(mice)
library(dummies)
library(Boruta)
library(pROC)
```

## Overview {.tabset .tabset-fade .tabset.-pills}


***
<center> **PROJECT SECTIONS** </center>
***

### Palmer Penguins 

#### Data Exploration 

The `palmerpenguins` data contains size measurements collected from 2007 - 2009 for three penguin species observed on three islands in the Palmer Archipelago, Antarctica. For more information about this data collection, refer to  [palmerpenguins website.](https://allisonhorst.github.io/palmerpenguins/articles/intro.html)

*Penguins Data Column Definition*

Variable | Description
----|------
species | penguin species (Ad√©lie, Chinstrap, and Gentoo)
island | island in Palmer Archipelago, Antarctica (Biscoe, Dream or Torgersen)
bill_length_mm | bill length (millimeters)
bill_depth_mm | bill depth (millimeters)
flipper_length_mm | flipper length (millimeters)
body_mass_g | body mass (grams)
sex | penguin sex (female, male)
year | year data was collected

```{r loadpeng}
# Load dataset
penguins = penguins

# Number of observations
ntrobs = dim(penguins)[[1]]

# Converting Year to factor
penguins$year = as.factor(penguins$year)
```

From the previous data exploration, it found that the response variable, `species` denotes one of three penguin species, and a majority of the penguins are Adelie (n = 153), followed by Gentoo (n = 124) and Chinstrap (n = 68). The distribution between gender is nearly equally divided among the species but not for their island habitat. 

```{r pengdist, fig.width=8}
reorder <- function(x){
  factor(x, levels = names(sort(table(x), decreasing = TRUE)))
}

ggplot(drop_na(penguins), aes(x = reorder(species), fill = species)) + 
  geom_bar() +
  geom_text(stat = "count", aes(label =..count..), vjust=-0.5, size = 3) +
  facet_wrap(~sex) +
  scale_fill_brewer(palette = "Accent") +
  theme_minimal() +
  theme(legend.position = "none")+
  labs(title = "Distibution of Species by Gender", y = "Frequency", x = "Species")
  
 ggplot(drop_na(penguins), aes(x = reorder(species), fill = species)) + 
  geom_bar() +
  geom_text(stat = "count", aes(label =..count..), vjust=-0.5, size = 3) +
  facet_wrap(~island) +
  scale_fill_brewer(palette = "Accent") +
  theme_minimal() +
  theme(legend.position = "none")+
  labs(title = "Distibution of Species by Island Habitat", y = "Frequency", x = "Species")
```

There were `r ntrobs` observations of 4 numeric predictor variables and 2-factor predictor variables, namely `island`, and `sex`.  There is also a `year` variable that is ignored in this analysis. The data set did not have complete cases, and there is a presence of bi- and tri-modal distributions which suggests that there are differences among the penguin species. 

Lastly, it is noted that Adelie and Chinstrap measurements overlap for all variables except bill length. This feature is a definitive variable that produces complete separation among the penguin species into groups. This perfectly discriminating variable will be removed to get a reasonable estimate for the variables that can predict the outcome variable.

```{r pengsumm}
dfSummary(penguins, plain.ascii = TRUE, style = "grid", graph.col = FALSE, footnote = NA)
```

#### Data Preparation 

The summary above indicates the amount of missing data the penguin data contains. It appears that more than 3% of the missing data was from the `sex` variable. This further suggests that nearly 97% were complete. There were no missingness patterns, and their overall proportion was not very extreme. As a result, missingness can be corrected by imputation.

Further exploration revealed that no variable seems to be strongly influenced by any outliers. An outlier is an observation that lies an abnormal distance from other values in a random sample. Outliers in the data could distort predictions and affect the accuracy, therefore, these would need to be corrected. 

To build a smaller model without predictors with extremely high correlations, it is best to reduce the number of predictors such that there were no absolute pairwise correlations above 0.90. The correlogram below graphically represents the correlations between the numeric predictor variables, when ignoring the missing variables. Most of the numeric variables were uncorrelated with one another, but there were a few highly correlated pairs. From the correlogram, the relationship between the `body_mass_g` and `flipper_length_mm` is a highly positive correlation, and within reason, as larger flippers would indicate an increase in body mass. There are some variables with moderate correlations, but their relationship is also intuitive. However, no relationship was too extreme, and it is clear that Adelie and Chinstrap overlap for all variable measurements except bill length. This feature is identified as the definitive variable that produces complete separation among the penguin species into groups.
 
```{r pengcorrgram, fig.height=5.5}
ggpairs(penguins, columns = 3:6, title = "Correlogram of Variables", 
        ggplot2::aes(color = species),
        progress = FALSE, 
        lower = list(continuous = wrap("smooth", alpha = 0.3, size = 0.1))) 
```

##### Training & Testing Split

The models were trained on the same approximately 70% of the data set, reserving 30% for validation of which model to select for the species class on the test set. This will allow for the test via cross-validation scheme of the model to tune parameters for optimal performance. 

```{r split}
# Create training and testing split
set.seed(525)
intrain = createDataPartition(penguins$species, p = 0.70, list = FALSE)

# Train & Test predictor variables
train_peng.p = penguins[intrain, -c(1,8)] # remove species, and year
test_peng.p = penguins[-intrain, -c(1,8)] 

# Train & Test response variable (species)
train_peng.r = penguins$species[intrain]
test_peng.r = penguins$species[-intrain]
```

##### Pre-Processing of Predictors

Missing data are treated by imputation. The classification and regression trees (CART) missing data algorithm was implemented because this could handle mixed types of missing data, and adaptable to interactions and non-linearity. 

```{r pengprepro}
set.seed(525)
temp = mice(train_peng.p, method = 'cart', print = FALSE, m = 3, maxit = 3)
train_peng.p = complete(temp)

temp = mice(test_peng.p, method = 'cart', print = FALSE, m = 3, maxit = 3)
test_peng.p = complete(temp)
```

##### Normality & Linearity 

The data were then pre-processed to fulfill the assumption of normality by centering and scaling.

```{r normality}
set.seed(525)
# Train set
processed_train_peng = preProcess(train_peng.p)
train_peng.p = predict(processed_train_peng, train_peng.p)

# Test set
processed_test_peng = preProcess(test_peng.p)
test_peng.p = predict(processed_test_peng, test_peng.p)
```

##### Dummy Variables

The categorical variables are then dummyfied. For instance, in the variable `sex`, the female will be used as the reference, whereas in the `island` variable, Biscoe island will be used as the reference.

```{r pengdummyVars}
set.seed(525)
# Train set
train_peng.pd = dummy.data.frame(train_peng.p, names = c("island","sex") , sep = ".")
train_peng.p = cbind(train_peng.p, train_peng.pd[,c(1:3,8:9)])
train_peng.p[sapply(train_peng.p, is.factor)] = data.matrix(train_peng.p[sapply(train_peng.p, is.factor)])
train_peng.p[,c(6:11)] = lapply(train_peng.p[,c(6:11)], factor) 
train_peng.p$island = factor(train_peng.p$island)
  
# Test set 
test_peng.pd = dummy.data.frame(test_peng.p, names = c("island","sex") , sep = ".")
test_peng.p = cbind(test_peng.p, test_peng.pd[,c(1:3,8:9)])
test_peng.p[sapply(test_peng.p, is.factor)] = data.matrix(test_peng.p[sapply(test_peng.p, is.factor)])
test_peng.p[,c(6:11)] = lapply(test_peng.p[,c(6:11)], factor) 
test_peng.p$island = factor(test_peng.p$island)
```

##### Feature Selection

To identify which features are important when building predictive model, feature selection is conducted to assist in choosing variables that are useful in predicting the response. The possible features that are impactful to classifying penguin species are listed below. This was done by using the random forest algorithm to performs a top-down search for relevant features and comparing the original attributes' importance with the importance achievable at random. It shows that `bill_length_mm` is indeed the most contributing variable followed by `flipper_length_mm`, and so on. 

```{r pengboruta}
output = Boruta(train_peng.r ~ ., data = train_peng.p, doTrace = 0)  
roughFixMod = TentativeRoughFix(output)
importance = attStats(TentativeRoughFix(output))
importance = importance[importance$decision != 'Rejected', c('meanImp', 'decision')]
kable(head(importance[order(-importance$meanImp), ])) %>%
  kable_styling(bootstrap_options = "striped", full_width = TRUE)
```

All in all, the following decision were made based on the feature selection investigation:

* The `flipper_length_mm` and `bill_depth_mm` are the most likely contributing variables that will be in the model.  
* The variable `island` is kept and evaluated per model on how much of a contribution difference it makes based on the algorithm and algorithm assumptions.  
* However, the `bill_length_mm` variable is removed due to it being a perfectly discriminating variable.   
* Due to high correlation with `flipper_length_mm`, `body_mass_g` is removed to avoid collinearity.  
* The `sex` variable is removed as it does not contribute based on the feature selection investigation.   
* The `year` variable is ignored.  

#### Building the Model

K-Nearest Neighbor classification works such that for each row of the test set, the k nearest (in Euclidean distance) training set vectors are found, and the classification is decided by majority vote, with ties broken at random.

With the decision on the features set, there is no need to perform stepwise elimination to account for the best predictors, thus to optimize each model, 10 repeats of 10-fold cross-validation is perform. With accuracy being the decision metric for the best performing model, there is 10 repeats of the 10-fold cross-validation. By doing this, the training set is divided randomly into 10 parts and then each of 10 parts is used as testing set for the model trained on other the 9. Then the average of the 10 error terms is obtained by performing the 10-fold CV ten times. The advantage of a repeated hold-out instead of a k-fold is that there have more control.

```{r pengknn}
set.seed(525)
knnModel = train(x = train_peng.p[, c(3:4)], 
                 y = train_peng.r,
                 method = "knn",
                 trControl = trainControl(method = "repeatedcv", 
                                          number = 10, 
                                          repeats = 10))
```

```{r pengmodel}
knnModel
```

```{r pengplot,fig.height=4, fig.width=8}
plot(knnModel, main = "Accuracy of KNN Model")
plot(varImp(knnModel), main = "Rank of Most Important Variable")
```

#### Model Discussion & Conclusions

The best tune for the KNN model which resulted in the largest accuracy is k = 5. It has accuracy = 80.5%, and $\kappa$ = 0.69. This tune accounts for the largest portion of the variability in the data than all other latent variables. Moreover, the variables that contributed the most to identifying Adelie and Gentoo is the flipper length, while bill depth was the most important variable to help classify Chinstrap. From the results based on the test data, the KNN model did exceptionally well in classifying the test set. Thus, the optimal model has an accuracy of 80.4% and $\kappa$ = 0.68 on the test set. 

In terms of the confusion matrix, the results suggest that 80.4% of the predicted results seem to be correctly classified. The precision for each type of species is also high (Adelie = 74%, Chinstrap = 54%, and Gentoo = 100%), suggesting that the penguins belong to the actual species among all the penguins predicted to be that particular species, with Gentoos being classified correctly 100% of the time. Moreover, the recall highlights that 87% of the Adelie species have been correctly classified accordingly, whereas 35% of the Chinstrap species have been correctly classified, and 97% of the Gentoo species have been correctly classified. In all, this model is capable of classifying penguins into one of the three species with great accuracy, particularly Gentoo species which was expected as their measurements were quite different. And lastly, the Kappa statistic of 0.68 suggests that the overall accuracy of this model is better than the expected random chance classifier's accuracy.

```{r pengCMx}
set.seed(525)
# Confusion Matrix
pred.R = predict(knnModel, newdata = test_peng.p, type = "raw")
confusion = confusionMatrix(pred.R, test_peng.r, mode = "everything")
confusion
```

Next, a receiver operating characteristic (ROC) analysis is shown in Figure 1. The area under the curve (AUC) for each class was estimated for observed penguin species and their predicted values by fitting the KNN model. The multi-class area under the curve for the predicted penguin species is the mean for all three AUC. It was computed to be 0.854. That is, there is a 85.4% chance that the model will be able to distinguish among the three penguin species. 

<center> Fig 1: ROC Curves of the KNN Model </center>

```{r pengROC}
predictions = as.numeric(predict(knnModel, test_peng.p, type = 'raw'))
roc.multi = multiclass.roc(test_peng.r, predictions)
auc(roc.multi)
plot.roc(roc.multi[['rocs']][[1]], main = "Multi-class ROC, Macro-Average ROC = 0.854")
sapply(2:length(roc.multi[['rocs']]), function(i) lines.roc(roc.multi[['rocs']][[i]], col=i))

legend("bottomright", 
       legend = c("ROC curve of Chinstrap",
                  "ROC curve of Gentoo",
                  "ROC curve of Adelie"), 
       col = c("black", "red", "green"), lwd = 2)

```

Given the `palmerpenguins` dataset, five multi-classification models, namely multinomial logistic regression, linear discriminant analysis, quadratic discriminant analysis, Naive Bayes and K-nearest neighbor models were fitted. From previous analysis, each model had its strengths and weakness, and in the end, the results were as follows:

Model | Accuracy
------|------
Multinomial Logistic Regression | 0.87
Linear Discriminant Analysis | 0.82
Quadratic Discriminant Analysis | 0.83
Naive Bayes | 0.92
K Nearest Neighbor | 0.80

Of all the classification model used to classify the penguin species, the Naive Bayes model's ability was proved to be near-optimal. Adelie and Gentoo were seen to be classified easily based on the flipper length, as it was the most important variable used in the classification. Whereas, for Chinshtrap, it was the bill depth. In conclusion, the Naive Bayes classifier produced a model that is 92.1% accurate in correctly classifying penguins into `Adelie`, `Chinstrap`, and `Gentoo`. This model also had an error rate of 0.161 between the measurements, which is the smallest than what the other models determined.

***

### Loan Approval 

The **loan approval** dataset will be used for the remaining models. 

```{r, include = FALSE}
loan_df_link <- 'https://raw.githubusercontent.com/greeneyefirefly/DATA622-Group3/main/Project_3/Loan_approval.csv'
loan_df <- read.csv(loan_df_link) %>%
  mutate_all(na_if,"") %>%
  mutate(Gender = as.factor(Gender),
         Married = as.factor(Married), 
         Dependents = as.factor(Dependents),
         Education = as.factor(Education),
         Self_Employed = as.factor(Self_Employed),
         Credit_History = as.factor(Credit_History),
         Property_Area = as.factor(Property_Area),
         Loan_Status = as.factor(Loan_Status)) 
loan_df = loan_df[,-1] 
```

This dataset includes `r nrow(loan_df)` datapoints and `r ncol(loan_df)` columns. The target variable is `Loan_Status`. Since the `Loan_ID` variable is unique to each record, we will remove it from the dataset. 

```{r}
str(loan_df)
```

We can also see that the `Loan_Status` classification is highly imbalanced, with more than double the amount of approvals (Y) than rejections (N). 

```{r}
loan_df %>% 
  count(Loan_Status)
```

#### Visualizing the Data
Let's take a preliminary look at the summary statistics for the dataset: 

```{r}
summary(loan_df)
```

Some things to note: 

* Seven of the variables have missing values, which is something we will have to deal with later on. 
* Almost all of the categorical variables are highly imbalanced: `Gender` (more males than females), `Married` (more married loan applicants than single), `Education` (more graduates than non-graduates), `Self_Employed` (less self-employed individuals), and `Credit History` (more individuals with credit history than not).
* At first glance, the `Applicant Income` variable looks to be a little right-skewed (higher mean than median).

##### Categorical Features

We'll look at each of the categorical features with respect to the final classification. Since most of our categorical features are imbalanced, we will look at the data in terms of percentages as opposed to counts. 

We can see that `Gender` doesn't appear to have as much of an impact on the final outcome. Regardless of the sex, around 70% of individuals are approved for a loan. 

```{r, echo=FALSE}
tab_gender <- with(loan_df, table(Gender, Loan_Status))
prop.table(tab_gender, margin = 1)
```

`Married`: Married individuals tend to be approved more often than non-married individuals. 

```{r, echo=FALSE}
tab_married <- with(loan_df, table(Married, Loan_Status))
prop.table(tab_married, margin = 1)
```

The number of `Dependents` an individual has doesn't appear to be as indicative of loan approval. 

```{r, echo=FALSE}
tab_kids <- with(loan_df, table(Dependents, Loan_Status))
prop.table(tab_kids, margin = 1)
```

`Education`: Graduates tend to be approved more often than non-graduates. 

```{r, echo = FALSE}
tab_edu <- with(loan_df, table(Education, Loan_Status))
prop.table(tab_edu, margin = 1)
```

`Self-Employed`: No real impact.

```{r, echo = FALSE}
tab_emp <- with(loan_df, table(Self_Employed, Loan_Status))
prop.table(tab_emp, margin = 1)
```

`Credit History`: This factor has a really large impact on the final approval! 79% of individuals with credit history are approved versus only 8% for those with no credit history. 

```{r, echo = FALSE}
tab_cred <- with(loan_df, table(Credit_History, Loan_Status))
prop.table(tab_cred, margin = 1)
```

`Property_Area`: Individuals living in semi-urbal areas tend to be approved more often than those in rural or urban areas.  

```{r ,echo = FALSE}
tab_prop <- with(loan_df, table(Property_Area, Loan_Status))
prop.table(tab_prop, margin = 1)
```

##### Numeric Features 

We can also take a look at the numeric features. 

```{r, fig.height=3, echo = FALSE}
loan_df %>%
  drop_na() %>%
  ggplot( aes(x=ApplicantIncome, fill=Loan_Status)) +
  geom_histogram( alpha=0.6, position = 'identity') +
  labs(title = 'Loan Approval by Applicant Income')
```

```{r, fig.height=3, echo = FALSE}
loan_df %>%
  drop_na() %>%
  ggplot( aes(x=CoapplicantIncome, fill=Loan_Status)) +
  geom_histogram( alpha=0.6, position = 'identity') +
  labs(title = 'Loan Approval by Coapplicant Income')
```

```{r, fig.height=3, echo = FALSE}
loan_df %>%
  drop_na() %>%
  ggplot( aes(x=LoanAmount, fill=Loan_Status)) +
  geom_histogram( alpha=0.6, position = 'identity') +
  labs(title = 'Loan Approval by Loan Amount')
```

```{r, fig.height=3, echo = FALSE}
loan_df %>%
  drop_na() %>%
  ggplot( aes(x=Loan_Amount_Term, fill=Loan_Status)) +
  geom_histogram( alpha=0.6, position = 'identity') +
  labs(title = 'Loan Approval by Loan Amount Term')
```

Finally, we can look at the correlations between the features. 

```{r}
loan_corr <- loan_df %>% drop_na() %>% select_if(is.numeric) %>% cor()
loan_corr[lower.tri(loan_corr, diag = TRUE)] <- NA
loan_corr
```

None of the numeric features are highly correlated with each other. 

#### Dealing with Missing Values

Similar to how we analyzed the missing values in the penguins dataset, we will look at the missing values in the loan dataset. This time, we'll use the summary function to get a gauge on this: 

```{r}
summary(loan_df)
```

We can see that 5 categorical and 2 numeric variables have missing values. 

* **Gender**, **Married**, and **Dependents** are not easily imputed, and since we have a small number of missing values, we will remove these records from the dataset. 


#### Training & Analysis

##### Model #1: Decision Trees

##### Model #2: Random Forest

##### Model #3: Gradient Boosting

#### Model Performance 

#### Conclusions


### Works Cited

1. Horst AM, Hill AP, Gorman KB (2020). *palmerpenguins: Palmer Archipelago (Antarctica) penguin data. R package version 0.1.0*. https://allisonhorst.github.io/palmerpenguins/. doi:10.5281/zenodo.3960218.

### Code Appendix

The code chunks below represent the R code called in order during the analysis. They are reproduced in the appendix for review and comment.

```{r appendix, include=FALSE}
knitr::opts_chunk$set(echo=TRUE, eval=FALSE)
```

```{r loadpeng}
```
```{r pengdist, fig.width=8}
```
```{r pengsumm}
```
```{r pengcorrgram, fig.height=5.5}
```
```{r split}
```
```{r pengprepro}
```
```{r normality}
```
```{r pengdummyVars}
```
```{r pengboruta}
```
```{r pengknn}
```
```{r pengmodel}
```
```{r pengplot,fig.height=4, fig.width=8}
```
```{r pengCMx}
```
```{r pengROC}
```
